{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":38321,"databundleVersionId":4196674,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install iterative-stratification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:54:57.839385Z","iopub.execute_input":"2025-01-05T04:54:57.839597Z","iopub.status.idle":"2025-01-05T04:55:02.422160Z","shell.execute_reply.started":"2025-01-05T04:54:57.839579Z","shell.execute_reply":"2025-01-05T04:55:02.421287Z"}},"outputs":[{"name":"stdout","text":"Collecting iterative-stratification\n  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (3.5.0)\nDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.9\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile config.py\nfrom transformers import AutoTokenizer\n\nTRAIN_BATCH_SIZE = 2\nVALID_BATCH_SIZE = 2\nEPOCHS = 5\nMAX_LEN = 512\nTRAINING_FILE = \"/kaggle/working/train_folds.csv\"\nMODEL_NAME = \"microsoft/deberta-v3-large\"\nMODEL_PATH = \"deberta_v3_large.bin\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:02.423929Z","iopub.execute_input":"2025-01-05T04:55:02.424155Z","iopub.status.idle":"2025-01-05T04:55:02.429626Z","shell.execute_reply.started":"2025-01-05T04:55:02.424138Z","shell.execute_reply":"2025-01-05T04:55:02.429025Z"}},"outputs":[{"name":"stdout","text":"Writing config.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile early_stopping.py\n\nimport config\nimport torch\n\nclass EarlyStopping:\n    def __init__(self, patience = 5, delta = 0.001, mode = \"min\"):\n        self.patience = patience\n        self.delta = delta\n        self.mode = mode\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n        self.save_path = config.MODEL_PATH\n\n    def __call__(self, val_metric, model):\n        score = -val_metric if self.mode == \"min\" else val_metric\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_metric, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_metric, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_metric, model):\n        torch.save(model.state_dict(), self.save_path)\n        print(f\"Model save with {val_metric: .4f} performance\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:02.431063Z","iopub.execute_input":"2025-01-05T04:55:02.431355Z","iopub.status.idle":"2025-01-05T04:55:02.444580Z","shell.execute_reply.started":"2025-01-05T04:55:02.431325Z","shell.execute_reply":"2025-01-05T04:55:02.443950Z"}},"outputs":[{"name":"stdout","text":"Writing early_stopping.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ndef create_folds(input_file_path, output_file_path, num_folds = 5):\n    df = pd.read_csv(input_file_path)\n    df[\"FOLD\"] = -1\n    target_cols = [\"cohesion\",\"syntax\",\"vocabulary\",\"phraseology\",\"grammar\",\"conventions\"]\n    skf = MultilabelStratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 42)\n    for i, (train_idx, val_idx) in enumerate(skf.split(df, df[target_cols])):\n        df.loc[val_idx, \"FOLD\"] = i+1\n\n    df.to_csv(output_file_path, index = False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:02.445241Z","iopub.execute_input":"2025-01-05T04:55:02.445422Z","iopub.status.idle":"2025-01-05T04:55:02.458864Z","shell.execute_reply.started":"2025-01-05T04:55:02.445405Z","shell.execute_reply":"2025-01-05T04:55:02.458047Z"}},"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import utils\n\nutils.create_folds(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\",\n                  \"train_folds.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:02.459730Z","iopub.execute_input":"2025-01-05T04:55:02.460012Z","iopub.status.idle":"2025-01-05T04:55:03.675924Z","shell.execute_reply.started":"2025-01-05T04:55:02.459982Z","shell.execute_reply":"2025-01-05T04:55:03.675245Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/working/train_folds.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.676848Z","iopub.execute_input":"2025-01-05T04:55:03.677154Z","iopub.status.idle":"2025-01-05T04:55:03.781653Z","shell.execute_reply.started":"2025-01-05T04:55:03.677124Z","shell.execute_reply":"2025-01-05T04:55:03.780837Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  003885A45F42  The best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  FOLD  \n0     3.5         3.0          3.0      4.0          3.0     2  \n1     2.5         3.0          2.0      2.0          2.5     1  \n2     3.5         3.0          3.0      3.0          2.5     5  \n3     4.5         4.5          4.5      4.0          5.0     4  \n4     3.0         3.0          3.0      2.5          2.5     2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>FOLD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df[\"full_text\"].apply(lambda x: len(x.split())).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.782506Z","iopub.execute_input":"2025-01-05T04:55:03.782729Z","iopub.status.idle":"2025-01-05T04:55:03.859222Z","shell.execute_reply.started":"2025-01-05T04:55:03.782710Z","shell.execute_reply":"2025-01-05T04:55:03.858485Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([261, 533, 320, ..., 257, 510, 638])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.859932Z","iopub.execute_input":"2025-01-05T04:55:03.860139Z","iopub.status.idle":"2025-01-05T04:55:03.864569Z","shell.execute_reply.started":"2025-01-05T04:55:03.860122Z","shell.execute_reply":"2025-01-05T04:55:03.863869Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(3911, 9)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# import importlib\n# import utils  # Import the script\n# importlib.reload(utils)  # Reload the updated script\n\n# input_file_path = \"/kaggle/input/feedback-prize-english-language-learning/train.csv\"\n# output_file_path = \"train_folds.csv\"\n# utils.create_folds(input_file_path, output_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.865362Z","iopub.execute_input":"2025-01-05T04:55:03.865643Z","iopub.status.idle":"2025-01-05T04:55:03.876080Z","shell.execute_reply.started":"2025-01-05T04:55:03.865623Z","shell.execute_reply":"2025-01-05T04:55:03.875339Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%%writefile dataset.py\nimport torch\n\nclass FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx][\"input_ids\"]\n        input_labels = self.samples[idx][\"input_labels\"]\n        input_ids = [self.tokenizer.cls_token_id] + ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        res = {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n            \"targets\": input_labels,\n        }\n        return res\n\n\nclass Collate:\n    def __init__(self, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n\n        batch_max_len = max([len(ids) for ids in output[\"ids\"]])\n        if batch_max_len > self.max_len:\n            batch_max_len = self.max_len\n\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max_len - len(s))*[self.tokenizer.pad_token_id]\n                            for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max_len - len(s))*[self.tokenizer.pad_token_id]\n                            for s in output[\"ids\"]]\n        else:\n            output[\"ids\"] = [(batch_max_len - len(s)) * [self.tokenizer.pad_token_id] + s \n                             for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max_len - len(s)) * [0] + s \n                              for s in output[\"mask\"]]\n\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype=torch.float)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.876907Z","iopub.execute_input":"2025-01-05T04:55:03.877183Z","iopub.status.idle":"2025-01-05T04:55:03.891053Z","shell.execute_reply.started":"2025-01-05T04:55:03.877150Z","shell.execute_reply":"2025-01-05T04:55:03.890193Z"}},"outputs":[{"name":"stdout","text":"Writing dataset.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile engine.py\nfrom sklearn.metrics import mean_squared_error\nfrom torch import nn\nfrom tqdm import tqdm\nimport torch\nimport numpy as np\nimport torch\n\ndef check_gpu_status():\n    num_gpus = torch.cuda.device_count()\n    for gpu_id in range(num_gpus):\n        device = torch.device(f\"cuda:{gpu_id}\")\n        total_memory = torch.cuda.get_device_properties(device).total_memory / 1024**2\n        allocated_memory = torch.cuda.memory_allocated(device) / 1024**2\n        reserved_memory = torch.cuda.memory_reserved(device) / 1024**2\n        free_memory = total_memory - reserved_memory\n        \n        print(f\"GPU {gpu_id}:\")\n        print(f\"  Total Memory:    {total_memory:.2f} MB\")\n        print(f\"  Allocated Memory:{allocated_memory:.2f} MB\")\n        print(f\"  Cached Memory:   {reserved_memory:.2f} MB\")\n        print(f\"  Free Memory:     {free_memory:.2f} MB\")\n        print(\"-\" * 30)\n\ndef loss_fn(outputs, targets):\n    loss_fct = nn.MSELoss()\n    loss = loss_fct(outputs, targets)\n    return loss\n\ndef monitor_metrics(outputs, targets):\n    device = targets.get_device()\n    outputs = outputs.detach().cpu().numpy()\n    targets = targets.detach().cpu().numpy()\n    num_labels = 6\n    mcrmse = []\n    for i in range(num_labels):\n        mcrmse.append(\n            mean_squared_error(\n                targets[:, i],\n                outputs[:, i],\n                squared = False\n            ),\n        )\n    mcrmse = np.mean(mcrmse)\n    return {\"mcrmse\": torch.tensor(mcrmse, device = device)}\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    final_loss = 0\n    print(\"In train_fn: \", len(data_loader))\n    for data in tqdm(data_loader, total = len(data_loader)):\n        \n        # torch.cuda.empty_cache()\n        for k, v in data.items():\n            data[k] = v.to(device)\n        optimizer.zero_grad()\n        _, loss, _ = model(**data)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        final_loss += loss.item()\n        # check_gpu_status()\n    return final_loss / len(data_loader)\n\n# def train_fn(data_loader, model, optimizer, device, scheduler):\n#     model.train()\n#     final_loss = 0\n#     print(\"In train_fn: \", len(data_loader))\n#     for data in tqdm(data_loader, total = len(data_loader)):\n        \n#         # torch.cuda.empty_cache()\n#         for k, v in data.items():\n#             data[k] = v.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(**data)\n#         if isinstance(outputs, tuple):\n#             loss = outputs[1]\n#         else:\n#             loss = outputs\n#         # Ensure loss is scalar\n#         if loss.dim() > 0:\n#             loss = loss.mean()\n        \n#         loss.backward()\n#         optimizer.step()\n#         scheduler.step()\n#         final_loss += loss.item()\n#     return final_loss / len(data_loader)\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    final_loss = 0\n    with torch.no_grad():  \n        for data in tqdm(data_loader, total = len(data_loader)):\n            for k, v in data.items():\n                data[k] = v.to(device)\n            _, loss, _ = model(**data)\n            final_loss += loss.item()\n    return final_loss / len(data_loader)\n\n\n# def eval_fn(data_loader, model, device):\n#     model.eval()\n#     final_loss = 0\n#     for data in tqdm(data_loader, total=len(data_loader)):\n#         for k, v in data.items():\n#             data[k] = v.to(device)\n            \n#         with torch.no_grad():\n#             outputs = model(**data)\n#             if isinstance(outputs, tuple):\n#                 loss = outputs[1]  # Assuming loss is the second element\n#             else:\n#                 loss = outputs\n                \n#             if loss.dim() > 0:\n#                 loss = loss.mean()\n                \n#         final_loss += loss.item()\n#     return final_loss / len(data_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.894345Z","iopub.execute_input":"2025-01-05T04:55:03.894559Z","iopub.status.idle":"2025-01-05T04:55:03.908965Z","shell.execute_reply.started":"2025-01-05T04:55:03.894536Z","shell.execute_reply":"2025-01-05T04:55:03.908264Z"}},"outputs":[{"name":"stdout","text":"Writing engine.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile model.py\nimport config\nimport engine\nfrom torch import nn\nfrom transformers import AutoConfig, AutoModel\n\nclass FeedbackModel(nn.Module):\n    def __init__(self, num_labels):\n        super().__init__()\n        self.num_labels = num_labels\n        hidden_dropout_prob: float = 0.0\n        model_config = AutoConfig.from_pretrained(config.MODEL_NAME)\n        model_config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"add_pooling_layer\": False,\n            \"num_labels\": self.num_labels\n        })\n        self.transformer = AutoModel.from_pretrained(config.MODEL_NAME,\n                                                    config = model_config)\n        self.dropout = nn.Dropout(model_config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(model_config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask, targets = None):\n        transformer_out = self.transformer(input_ids = ids, \n                                          attention_mask = mask)\n        sequence_output = transformer_out.last_hidden_state[:, 0, :]\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n\n        loss = 0\n\n        if targets is not None:\n            loss1 = engine.loss_fn(logits1, targets)\n            loss2 = engine.loss_fn(logits2, targets)\n            loss3 = engine.loss_fn(logits3, targets)\n            loss4 = engine.loss_fn(logits4, targets)\n            loss5 = engine.loss_fn(logits5, targets)\n            loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n            mcrmse = engine.monitor_metrics(logits, targets)\n            return logits, loss, mcrmse\n        return logits, loss, {}\n            \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.910091Z","iopub.execute_input":"2025-01-05T04:55:03.910343Z","iopub.status.idle":"2025-01-05T04:55:03.925162Z","shell.execute_reply.started":"2025-01-05T04:55:03.910325Z","shell.execute_reply":"2025-01-05T04:55:03.924390Z"}},"outputs":[{"name":"stdout","text":"Writing model.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile train.py\n\nimport config\nimport pandas as pd\nimport numpy as np\nimport torch\nimport engine\nfrom model import FeedbackModel\nfrom early_stopping import EarlyStopping\nfrom torch.utils.data import RandomSampler\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nfrom dataset import Collate, FeedbackDataset\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport subprocess\n\n# def check_gpu_status():\n#     result = subprocess.run(['nvidia-smi'], capture_output = True, text = True)\n#     print(result.stdout)\n\n\ndef _prepare_data_helper(tokenizer, df, text_ids):\n    samples = []\n    lbls = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n    for idx in tqdm(text_ids):\n        full_text = df[df.text_id == idx].reset_index(drop=True).full_text.values[0]\n        encoded_text = tokenizer.encode_plus(\n            full_text,\n            None,\n            add_special_tokens=False,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        sample = {\n            \"input_ids\": input_ids,\n            \"text_id\": idx,\n            \"full_text\": full_text,\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n            \"input_labels\": df[df.text_id == idx].reset_index(drop=True)[lbls].values[0, :].tolist(),\n        }\n        samples.append(sample)\n\n    return samples\n    \n\ndef prepare_data(df, tokenizer, num_jobs):\n    samples = []\n    text_ids = df[\"text_id\"].unique()\n    text_ids_splits = np.array_split(text_ids, num_jobs)\n    results = Parallel(n_jobs = num_jobs, backend=\"multiprocessing\")(\n        delayed(_prepare_data_helper)(tokenizer, df, idx)\n        for idx in text_ids_splits\n    )\n    for result in results:\n        samples.extend(result)\n    return samples\n\ndef run(fold):\n    NUM_JOBS = 12\n    df = pd.read_csv(config.TRAINING_FILE)\n    target_columns = [\"cohesion\",\"syntax\",\"vocabulary\",\"phraseology\",\"grammar\",\"conventions\"]\n    train_dataset = df[df[\"FOLD\"] != fold].reset_index(drop = True)\n    valid_dataset = df[df[\"FOLD\"] == fold].reset_index(drop = True)\n    training_samples = prepare_data(train_dataset, config.tokenizer, num_jobs = NUM_JOBS)\n    valid_samples = prepare_data(valid_dataset, config.tokenizer, num_jobs = NUM_JOBS)\n    print(len(valid_samples))\n    num_train_steps = int(len(train_dataset) / config.TRAIN_BATCH_SIZE *  config.EPOCHS)\n    n_gpu = torch.cuda.device_count()\n    if n_gpu > 0:\n        num_train_steps /= n_gpu\n\n    train_dataset = FeedbackDataset(training_samples, config.MAX_LEN, config.tokenizer)\n    valid_dataset = FeedbackDataset(valid_samples, config.MAX_LEN, config.tokenizer)\n\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n                                                   batch_size=config.TRAIN_BATCH_SIZE,\n                                                   collate_fn=Collate(config.tokenizer, config.MAX_LEN)\n                                                  )\n    valid_dataloader = torch.utils.data.DataLoader(valid_dataset,\n                                                   batch_size=config.VALID_BATCH_SIZE,\n                                                   collate_fn=Collate(config.tokenizer, config.MAX_LEN)\n                                                  )\n    print(\"Training Length: \", len(train_dataloader))\n    print(\"Validation Length: \", len(valid_dataloader))\n\n    # model =  torch.nn.parallel.DistributedDataParallel(FeedbackModel(num_labels=len(target_columns)))\n    model =  FeedbackModel(num_labels=len(target_columns))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    model.to(device)\n\n    # check_gpu_status()\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0\n        }\n    ]\n\n    optimizer = AdamW(optimizer_parameters, lr = 3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n    )\n\n    early_stopping = EarlyStopping()\n    for epoch in range(config.EPOCHS):\n        # check_gpu_status()\n        train_loss = engine.train_fn(train_dataloader, model, optimizer, device, scheduler)\n        val_loss = engine.eval_fn(valid_dataloader, model, device)\n        print(f\"Train loss = {train_loss} Valid loss = {val_loss}\")\n\n        early_stopping(val_loss, model)\n\n        # Check if we should stop training\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.925966Z","iopub.execute_input":"2025-01-05T04:55:03.926234Z","iopub.status.idle":"2025-01-05T04:55:03.942464Z","shell.execute_reply.started":"2025-01-05T04:55:03.926206Z","shell.execute_reply":"2025-01-05T04:55:03.941655Z"}},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import importlib\nimport config \nimport train\nimport model\nimport dataset\nimport early_stopping\nimport engine\nimport utils\nimportlib.reload(config)  \nimportlib.reload(train)\nimportlib.reload(model)\nimportlib.reload(dataset)  \nimportlib.reload(early_stopping)\nimportlib.reload(engine)\nimportlib.reload(utils)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:03.943204Z","iopub.execute_input":"2025-01-05T04:55:03.943453Z","iopub.status.idle":"2025-01-05T04:55:10.868025Z","shell.execute_reply.started":"2025-01-05T04:55:03.943433Z","shell.execute_reply":"2025-01-05T04:55:10.867124Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26670b160794be49d3b95ae2274064c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d9d65ff9c343b0840e4ba2be6814c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2436e72fe84acc84e3cd018aa40719"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<module 'utils' from '/kaggle/working/utils.py'>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# torch.cuda.memory_reserved()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:10.868822Z","iopub.execute_input":"2025-01-05T04:55:10.869207Z","iopub.status.idle":"2025-01-05T04:55:10.872414Z","shell.execute_reply.started":"2025-01-05T04:55:10.869186Z","shell.execute_reply":"2025-01-05T04:55:10.871690Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import train\nfold = 5\ntrain.run(fold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:55:10.873222Z","iopub.execute_input":"2025-01-05T04:55:10.873454Z","iopub.status.idle":"2025-01-05T06:13:34.907889Z","shell.execute_reply.started":"2025-01-05T04:55:10.873435Z","shell.execute_reply":"2025-01-05T06:13:34.907086Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 261/261 [00:00<00:00, 302.37it/s]\n100%|██████████| 261/261 [00:01<00:00, 209.05it/s]\n100%|██████████| 261/261 [00:01<00:00, 204.80it/s]\n100%|██████████| 261/261 [00:01<00:00, 202.60it/s]\n100%|██████████| 261/261 [00:01<00:00, 182.09it/s]\n100%|██████████| 261/261 [00:01<00:00, 194.22it/s]\n100%|██████████| 261/261 [00:01<00:00, 196.30it/s]\n100%|██████████| 261/261 [00:01<00:00, 200.11it/s]\n100%|██████████| 261/261 [00:01<00:00, 185.64it/s]\n100%|██████████| 260/260 [00:01<00:00, 224.41it/s]\n100%|██████████| 260/260 [00:01<00:00, 247.55it/s]\n100%|██████████| 260/260 [00:00<00:00, 279.24it/s]\n100%|██████████| 66/66 [00:00<00:00, 301.38it/s]\n100%|██████████| 66/66 [00:00<00:00, 340.50it/s]\n100%|██████████| 65/65 [00:00<00:00, 374.23it/s]\n100%|██████████| 65/65 [00:00<00:00, 353.92it/s]\n100%|██████████| 65/65 [00:00<00:00, 350.16it/s]\n100%|██████████| 65/65 [00:00<00:00, 339.84it/s]\n100%|██████████| 65/65 [00:00<00:00, 331.42it/s]\n100%|██████████| 65/65 [00:00<00:00, 333.59it/s]\n100%|██████████| 65/65 [00:00<00:00, 332.62it/s]\n100%|██████████| 65/65 [00:00<00:00, 333.99it/s]\n100%|██████████| 65/65 [00:00<00:00, 304.39it/s]\n100%|██████████| 65/65 [00:00<00:00, 311.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"782\nTraining Length:  1565\nValidation Length:  391\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d1d08e2f7d4ab9be648f48007d189b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"In train_fn:  1565\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1565/1565 [14:26<00:00,  1.81it/s]\n100%|██████████| 391/391 [01:06<00:00,  5.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss = 0.5681609327277055 Valid loss = 0.48474608740919384\nModel save with  0.4847 performance\nIn train_fn:  1565\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1565/1565 [14:26<00:00,  1.81it/s]\n100%|██████████| 391/391 [01:06<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss = 0.4937841117334442 Valid loss = 0.455505867424371\nModel save with  0.4555 performance\nIn train_fn:  1565\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1565/1565 [14:26<00:00,  1.81it/s]\n100%|██████████| 391/391 [01:06<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss = 0.4754387193903946 Valid loss = 0.4483734165387385\nModel save with  0.4484 performance\nIn train_fn:  1565\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1565/1565 [14:26<00:00,  1.81it/s]\n100%|██████████| 391/391 [01:06<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss = 0.4668452476041195 Valid loss = 0.4413993208647689\nModel save with  0.4414 performance\nIn train_fn:  1565\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1565/1565 [14:26<00:00,  1.81it/s]\n100%|██████████| 391/391 [01:06<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss = 0.46140148656341595 Valid loss = 0.43996008207349824\nModel save with  0.4400 performance\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!zip large_file.zip /kaggle/working/deberta_v3_large.bin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T06:22:21.530858Z","iopub.execute_input":"2025-01-05T06:22:21.531147Z","iopub.status.idle":"2025-01-05T06:25:11.870864Z","shell.execute_reply.started":"2025-01-05T06:22:21.531129Z","shell.execute_reply":"2025-01-05T06:25:11.869614Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/deberta_v3_large.bin (deflated 16%)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Compress the model file\n!zip -r model.zip /kaggle/working/deberta_v3_large.bin\n# Then create download link for the zip\nFileLink('model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T06:31:50.363155Z","iopub.execute_input":"2025-01-05T06:31:50.363451Z","iopub.status.idle":"2025-01-05T06:34:40.405072Z","shell.execute_reply.started":"2025-01-05T06:31:50.363431Z","shell.execute_reply":"2025-01-05T06:34:40.404161Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/deberta_v3_large.bin (deflated 16%)\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}